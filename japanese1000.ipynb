{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping common Japanese words with examples\n",
    "\n",
    "The goal is to obtain a csv file with data scraped from https://iknow.jp/courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "#https://www.linkedin.com/pulse/scraping-therapists-python-selenium-beautifulsoup-scott-edenbaum\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import lxml\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "\n",
    "def scrape_japanese(url):\n",
    "    driver = webdriver.Chrome(\"C:/Users/karol/drivers/chromedriver\")  #select selenium web driver\n",
    "    driver.get(url) #open the url in selenium\n",
    "    jpVocabTable = [] #this list will be populated with scraped text\n",
    "    soup = BeautifulSoup(driver.page_source,'html5lib') #grab the content with beautifulsoup for parsing\n",
    "    main_table = soup.findAll(\"li\",{\"class\":\"item\"})  # select the desired html node\n",
    "\n",
    "    for row in main_table:\n",
    "        rowList = []\n",
    "        \n",
    "        ######## find japanese words ###########################\n",
    "        \n",
    "        href = row.contents[0].contents[2].contents[0].find('a', href=True)\n",
    "        word = row.find_all('a', href=True)\n",
    "        jpWord= href.text.replace('\\'', '')\n",
    "        \n",
    "        ######## find transliteration ###########################\n",
    "        \n",
    "        translit = row.contents[0].contents[2].contents[0].find('span', {\"class\":\"transliteration\"})\n",
    "        if translit is not None:\n",
    "            comments = translit.findAll(text=lambda text:isinstance(text, Comment))\n",
    "            for comment in comments:\n",
    "                comment.extract()\n",
    "            transliteration = re.sub('\\[|\\]', '', translit.text)\n",
    "        else: #there are cases where there is no transliteration and we want them empty\n",
    "            transliteration = \"\"\n",
    "\n",
    "        ######## find English translation ###########################\n",
    "\n",
    "        trans = row.contents[0].contents[2].find('p', {\"class\":\"response\"})\n",
    "        #if trans is not None:  #some returning nones\n",
    "        translation = trans.text.replace('\\'', '')\n",
    "\n",
    "         ######## find example sentence 1 ###########################\n",
    "\n",
    "        sen1 = row.contents[1].contents[0].contents[2].find('span').text\n",
    "\n",
    "        ######## find example sentence 1 transliteration ###########################\n",
    "\n",
    "        sen1transli = row.contents[1].contents[0].contents[2].find('p', {\"class\":\"transliteration\"}).text\n",
    "\n",
    "        ######## find example sentence 1 translation ###########################\n",
    "\n",
    "        sen1transla = row.contents[1].contents[0].contents[2].find('p', {\"class\":\"translation\"}).text\n",
    "\n",
    "        ######## find example sentence 2 ###########################\n",
    "        try:\n",
    "            sen2 = row.contents[1].contents[1].contents[2].find('span').text\n",
    "        except IndexError as e:\n",
    "            print(e)\n",
    "\n",
    "        ######## find example sentence 2 transliteration ###########################\n",
    "        try:\n",
    "            sen2transli = row.contents[1].contents[1].contents[2].find('p', {\"class\":\"transliteration\"}).text\n",
    "        except IndexError as e:\n",
    "            print(e)\n",
    "\n",
    "        ######## find example sentence 2 translation ###########################\n",
    "        try:\n",
    "            sen2transla = row.contents[1].contents[1].contents[2].find('p', {\"class\":\"translation\"}).text\n",
    "        except IndexError as e:\n",
    "            print(e)\n",
    "\n",
    "\n",
    "        rowList.extend([jpWord,transliteration, translation, sen1, sen1transli, sen1transla, sen2, sen2transli, sen2transla])\n",
    "        jpVocabTable.append(rowList)\n",
    "            \n",
    "    driver.close()    \n",
    "    print(\"Scraping Complete!\")\n",
    "    return(jpVocabTable)\n",
    "        \n",
    "        \n",
    "url = \"https://iknow.jp/courses/566921\" \n",
    "japanese_list = scrape_japanese(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(japanese_list)\n",
    "df.columns = ['word', 'kana', 'translation', 'sentence1', 'sen1kana', 'sen1trans','sentence2','sen2kana', 'sen2trans']\n",
    "df.to_csv('japaneseWords_100.csv', encoding='utf8', index = False) #don't add an extra line for indexing\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['体',\n",
       "  'からだ',\n",
       "  'body, physique, physical condition',\n",
       "  '私は体が丈夫だ。',\n",
       "  'わたし は からだ が じょうぶ だ。',\n",
       "  \"I'm physically strong.\",\n",
       "  'タバコは体に悪い。',\n",
       "  'タバコ は からだ に わるい。',\n",
       "  'Cigarettes are bad for your health.']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_row = df.sample(n=1)\n",
    "list_row =random_row.values.tolist()\n",
    "list_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://iknow.jp/courses/566921',\n",
       " 'https://iknow.jp/courses/566922',\n",
       " 'https://iknow.jp/courses/566924',\n",
       " 'https://iknow.jp/courses/566925',\n",
       " 'https://iknow.jp/courses/566926',\n",
       " 'https://iknow.jp/courses/566927',\n",
       " 'https://iknow.jp/courses/566928',\n",
       " 'https://iknow.jp/courses/566929',\n",
       " 'https://iknow.jp/courses/566930',\n",
       " 'https://iknow.jp/courses/566932']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now, repeat scraping for all 1000 words, saving them in separate files\n",
    "#Note the pages do not follow consecurive numbers https://iknow.jp/courses/566921\n",
    "#prepare URLs\n",
    "def createUrlList():\n",
    "    urlFront = \"https://iknow.jp/courses/5669\"\n",
    "    urlEnds = [\"21\", \"22\", \"24\", \"25\", \"26\", \"27\", \"28\", \"29\", \"30\", \"32\"]\n",
    "    urlList = []\n",
    "    for element in urlEnds:\n",
    "        newListelement = urlFront+element\n",
    "        urlList.append(newListelement)\n",
    "    return urlList\n",
    "\n",
    "urls = createUrlList()\n",
    "urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running my function for other vocabulary pages, I got many \"list index out of range errors\". Sentences of interest were not always in the same place in each of the documents. I ran a few examples and noticed there are 2 possible tree structures. I rewrote my scraping program to check the length of the list, and then depending on the number of list elements retrieve the last element. This could be rewriten in an even nicer way, so it's not hard-coded (get element 2 or 3) but get the last element in a list of a any size. SOmething to improve in the future. Here is my new function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "#https://www.linkedin.com/pulse/scraping-therapists-python-selenium-beautifulsoup-scott-edenbaum\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import lxml\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "\n",
    "def scrape_japanese_all(url):\n",
    "    driver = webdriver.Chrome(\"C:/Users/karol/drivers/chromedriver\")  #select selenium web driver\n",
    "    driver.get(url) #open the url in selenium\n",
    "    jpVocabTable = [] #this list will be populated with scraped text\n",
    "    soup = BeautifulSoup(driver.page_source,'html5lib') #grab the content with beautifulsoup for parsing\n",
    "    main_table = soup.findAll(\"li\",{\"class\":\"item\"})  # select the desired html node\n",
    "\n",
    "    for row in main_table:\n",
    "        rowList = []\n",
    "        \n",
    "        ######## find japanese words ###########################\n",
    "        \n",
    "        href = row.contents[0].contents[2].contents[0].find('a', href=True)\n",
    "        word = row.find_all('a', href=True)\n",
    "        jpWord= href.text.replace('\\'', '')\n",
    "        \n",
    "        ######## find transliteration ###########################\n",
    "        \n",
    "        translit = row.contents[0].contents[2].contents[0].find('span', {\"class\":\"transliteration\"})\n",
    "        if translit is not None:\n",
    "            comments = translit.findAll(text=lambda text:isinstance(text, Comment))\n",
    "            for comment in comments:\n",
    "                comment.extract()\n",
    "            transliteration = re.sub('\\[|\\]', '', translit.text)\n",
    "        else: #there are cases where there is no transliteration and we want them empty\n",
    "            transliteration = \"\"\n",
    "\n",
    "        ######## find English translation ###########################\n",
    "\n",
    "        trans = row.contents[0].contents[2].find('p', {\"class\":\"response\"})\n",
    "        #if trans is not None:  #some returning nones\n",
    "        translation = trans.text.replace('\\'', '')\n",
    "        \n",
    "        ######## find example sentence 1 ###########################\n",
    "        \n",
    "        #create temporary list for find its length\n",
    "        temp_list_sen1 = row.contents[1].contents[0].contents\n",
    "        \n",
    "        # depending on length, ask for the last element       \n",
    "        if len(temp_list_sen1) ==2 :\n",
    "            sen1 = row.contents[1].contents[0].contents[1].find('span').text\n",
    "           # print(sen2)\n",
    "            sen1transli = row.contents[1].contents[0].contents[1].find('p', {\"class\":\"transliteration\"}).text\n",
    "            sen1transla = row.contents[1].contents[0].contents[1].find('p', {\"class\":\"translation\"}).text\n",
    "        elif len(temp_list_sen1) ==3 :\n",
    "            sen1 = row.contents[1].contents[0].contents[2].find('span').text\n",
    "           # print(sen2)\n",
    "            sen1transli = row.contents[1].contents[0].contents[2].find('p', {\"class\":\"transliteration\"}).text\n",
    "            sen1transla = row.contents[1].contents[0].contents[2].find('p', {\"class\":\"translation\"}).text\n",
    "        else:\n",
    "            print(\"different length\")\n",
    "\n",
    "        ######## find example sentence 2 ###########################\n",
    "    \n",
    "        temp_list_sen2 = row.contents[1].contents[1].contents\n",
    "        \n",
    "        if len(temp_list_sen2) ==2 :\n",
    "            sen2 = row.contents[1].contents[1].contents[1].find('span').text\n",
    "           # print(sen2)\n",
    "            sen2transli = row.contents[1].contents[1].contents[1].find('p', {\"class\":\"transliteration\"}).text\n",
    "            sen2transla = row.contents[1].contents[1].contents[1].find('p', {\"class\":\"translation\"}).text\n",
    "        elif len(temp_list_sen2) ==3 :\n",
    "            sen2 = row.contents[1].contents[1].contents[2].find('span').text\n",
    "           # print(sen2)\n",
    "            sen2transli = row.contents[1].contents[1].contents[2].find('p', {\"class\":\"transliteration\"}).text\n",
    "            sen2transla = row.contents[1].contents[1].contents[2].find('p', {\"class\":\"translation\"}).text\n",
    "        else:\n",
    "            print(\"different length\")\n",
    "            \n",
    "        #add scraped text to list\n",
    "        rowList.extend([jpWord,transliteration, translation, sen1, sen1transli, sen1transla, sen2, sen2transli, sen2transla])\n",
    "        jpVocabTable.append(rowList)\n",
    "            \n",
    "    driver.close()    \n",
    "    print(\"Scraping Complete!\")\n",
    "    return(jpVocabTable)\n",
    "        \n",
    "        \n",
    "#url = \"https://iknow.jp/courses/566927\" \n",
    "#japanese_list = scrape_japanese(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Complete!\n",
      "Scraping Complete!\n",
      "Scraping Complete!\n",
      "Scraping Complete!\n",
      "Scraping Complete!\n",
      "Scraping Complete!\n",
      "Scraping Complete!\n",
      "Scraping Complete!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length mismatch: Expected axis has 0 elements, new values have 9 elements",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-424e5548f29a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# # 4 pear\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m#########################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mgetAllVocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m#this will produce all 10 csv files with 1000 most common japanese words with examples.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-424e5548f29a>\u001b[0m in \u001b[0;36mgetAllVocab\u001b[1;34m(pages_list)\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mjapList\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscrape_japanese_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mjapDf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjapList\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mjapDf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'word'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'kana'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'translation'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sentence1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sen1kana'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sen1trans'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'sentence2'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'sen2kana'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sen2trans'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mcsvFileName\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'japaneseWords_100_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mjapDf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsvFileName\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#don't add an extra line for indexing\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__setattr__\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m   3092\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3093\u001b[0m             \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3094\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3095\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3096\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/src\\properties.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.AxisProperty.__set__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_set_axis\u001b[1;34m(self, axis, labels)\u001b[0m\n\u001b[0;32m    471\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_set_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    474\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    475\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mset_axis\u001b[1;34m(self, axis, new_labels)\u001b[0m\n\u001b[0;32m   2834\u001b[0m             raise ValueError('Length mismatch: Expected axis has %d elements, '\n\u001b[0;32m   2835\u001b[0m                              \u001b[1;34m'new values have %d elements'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2836\u001b[1;33m                              (old_len, new_len))\n\u001b[0m\u001b[0;32m   2837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2838\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Length mismatch: Expected axis has 0 elements, new values have 9 elements"
     ]
    }
   ],
   "source": [
    "# Run scraper for each URl\n",
    "def getAllVocab(pages_list):   \n",
    "    for counter, page in enumerate(pages_list, 1):\n",
    "        japList = scrape_japanese_all(page)\n",
    "        japDf = pd.DataFrame(japList)\n",
    "        japDf.columns = ['word', 'kana', 'translation', 'sentence1', 'sen1kana', 'sen1trans','sentence2','sen2kana', 'sen2trans']\n",
    "        csvFileName = 'japaneseWords_100_' + str(counter) + '.csv'\n",
    "        japDf.to_csv(csvFileName, encoding='utf8', index = False) #don't add an extra line for indexing\n",
    "\n",
    "        \n",
    "######################### enumerate example ############\n",
    "\n",
    "# my_list = ['apple', 'banana', 'grapes', 'pear']\n",
    "# for c, value in enumerate(my_list, 1):\n",
    "#     print(c, value)\n",
    "\n",
    "# # Output:\n",
    "# # 1 apple\n",
    "# # 2 banana\n",
    "# # 3 grapes\n",
    "# # 4 pear\n",
    "#########################################################\n",
    "getAllVocab(urls)\n",
    "\n",
    "#this will produce all 10 csv files with 1000 most common japanese words with examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The program doesn't always scrape the page properly (not sure why).It fails on various pages, with the: \"Length mismatch: Expected axis has 0 elements, new values have 9 elements\", the list doesn't get populated at all, on different runs it happens to various pages. I managed to get the 9 csvs, the last one will be done manually below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Complete!\n"
     ]
    }
   ],
   "source": [
    "        japList = scrape_japanese_all('https://iknow.jp/courses/566932')\n",
    "        japDf = pd.DataFrame(japList)\n",
    "        japDf.columns = ['word', 'kana', 'translation', 'sentence1', 'sen1kana', 'sen1trans','sentence2','sen2kana', 'sen2trans']\n",
    "        csvFileName = 'japaneseWords_100_10.csv'\n",
    "        japDf.to_csv(csvFileName, encoding='utf8', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
